# 시뮬레이션 기반 VLA/ACT 학습 데이터 생성 방법론

> **한 줄 요약**: D2E(Desktop to Embodied AI) 프레임워크에 기반하여, AI가 생성한 가정집 3D 게임 환경에서 레고 정리 태스크의 일반/롱테일 시나리오를 무한 생성하고, 다시점 비디오 + 연속 액션 데이터를 동시 기록하여 ACT/VLA 모델을 학습시킨다.

---

## 1. 핵심 전략: "AI가 게임을 만들고, 게임에서 학습 데이터를 생성한다"

### 1.1 이론적 기반: D2E (Desktop to Embodied AI)

본 프로젝트는 WoRV AI의 D2E 프레임워크(https://worv-ai.github.io/d2e/)에 근거한다.

**D2E의 핵심 통찰**: "대규모 텍스트 데이터로 LLM을 학습시키듯, 디지털 환경(게임)의 풍부한 감각-운동 상호작용 데이터로 로봇을 학습시킬 수 있다."

**D2E 파이프라인**:
- OWA 툴킷: 31개 게임에서 335.6시간 시연 수집, 152배 압축
- Generalist-IDM: 다양한 게임에서 제로샷 일반화, YouTube 1,000시간+ 자동 라벨링
- VAPT(시각-행동 사전학습): 데스크톱 표현 → 로봇 작업 전이

**검증된 성과**:
- LIBERO 조작 작업: 96.6% 성공률
- CANVAS 네비게이션: 83.3% 성공률
- 실제 SO101 로봇: 70% → 80% 향상

이는 **"게임의 감각-운동 패턴이 물리 로봇 작업으로 의미 있게 전이된다"**는 가설을 실증한다.

### 1.2 우리의 차별점: AI가 게임 자체를 생성한다

D2E는 기존 게임에서 데이터를 수집한다. 우리는 한 단계 더 나아간다:

```
기존 D2E:     기존 게임 → 데이터 수집 → 모델 학습
우리의 접근:  AI가 게임 생성 → 데이터 수집 → 모델 학습
             ↑ Claude/LLM이 Three.js 코드를 생성하여 시뮬레이션 환경을 자동 구축
```

**왜 이것이 강력한가:**
- 타겟 태스크에 최적화된 환경을 무한히 생성 가능
- 롱테일 시나리오를 의도적으로 설계 가능 (기존 게임은 있는 그대로만 사용)
- 환경 자체를 도메인 랜덤화 가능 (방 구조, 가구 배치, 조명 등)
- 게임 개발 비용이 사실상 제로 (AI가 코드 생성)

### 1.3 왜 Three.js인가 (기술 스택 선택 근거)

Three.js + cannon-es를 선택한 이유는 **D2E의 "게임에서 학습"이라는 패러다임**과 **"AI가 게임을 생성"이라는 전략**에 최적화되어 있기 때문이다.

**AI 코드 생성 친화성**
- Claude/GPT 등 LLM이 Three.js 코드를 높은 품질로 생성 가능
- MuJoCo XML/MJCF는 LLM이 상대적으로 약함
- 게임 환경 전체를 AI가 자동 생성하는 워크플로우에 Three.js가 압도적 우위

**빠른 프로토타이핑**
- 브라우저에서 즉시 확인 가능 (개발-검증 사이클 최단)
- npm 생태계의 풍부한 3D 에셋/라이브러리
- GLTF/GLB 모델 즉시 로딩

**D2E 정합성**
- D2E 원 논문도 게임(디지털 환경)에서 데이터를 수집
- 물리 정확도보다 감각-운동 패턴의 다양성이 중요
- 게임 수준의 물리(cannon-es)로도 LIBERO 96.6% 달성

**MuJoCo/Isaac Sim 대비 트레이드오프**
- MuJoCo: 물리 정확도 높음, 하지만 AI 생성 어렵고 시각적 품질 낮음
- Isaac Sim: 최고 품질, 하지만 무겁고 라이선스/설치 복잡
- Three.js: 물리 정확도는 게임 수준이지만, AI 생성 + 빠른 반복 + 시각적 품질의 균형점

**결론**: sim-to-real이 아닌 D2E(desktop-to-embodied) 접근에서는, 물리 정확도보다 환경 다양성과 데이터 생성 속도가 핵심이다. Three.js가 이 목적에 최적이다.

### 1.4 배경과 동기

실제 로봇으로 가정집에서 데이터를 수집하면:
- 환경 재현이 어렵고, 아이/반려동물 안전 문제가 있다
- 라벨링(정확한 action 기록)이 불완전하다
- 롱테일 케이스(강아지가 레고를 물고 가기, 아이가 갑자기 뛰어들기 등)를 의도적으로 만들기 힘들다

**대안: 가정집 환경을 게임으로 구축 (D2E 패러다임)**

```
실제 세계                          게임(시뮬레이션) 세계
────────                          ──────────────
데이터 수집 = 안전 문제            무한 생성, 안전
라벨링 = 수동, 불완전              자동, 완벽
롱테일 = 우연에 의존               의도적 설계
게임 개발 = 비싸고 느림            AI가 자동 생성
Sim-to-Real gap                   D2E로 검증된 전이 가능성
```

### 1.5 추가 이론적 근거

- **Domain Randomization**: 환경 파라미터(조명, 텍스처, 배치)를 충분히 랜덤화하면 실제 환경도 랜덤화 범위 안에 포함됨 (Tobin et al., 2017)
- **ACT의 데이터 효율성**: Action Chunking with Transformers는 수백 개 에피소드로도 유의미한 policy 학습 가능 (Zhao et al., 2023)
- **VLA 사전학습**: 대규모 비디오+액션 데이터로 사전학습한 VLA는 소량의 실제 데이터로 fine-tuning 가능 (Brohan et al., RT-2)
- **합성 데이터 성공 사례**: NVIDIA Isaac Sim, Google 등에서 시뮬레이션 → 실제 로봇 전이 검증 완료

### 1.6 왜 레고 정리인가

- **투자자/파트너에게 직관적**: 누구나 아이가 레고를 어질러놓는 상황을 이해
- **다양한 난이도**: 단순 줍기부터 가구 사이 꺼내기, 색상별 분류까지 자연스러운 확장
- **실생활 문제**: 가정용 로봇의 핵심 유즈케이스와 직결
- **기존 레고 게임 자산 활용**: 이미 구축된 3D 레고 블록 모델 재사용

---

## 2. 환경 설계: 가정집

### 2.1 공간 구성

```
┌──────────────────────────────────────────────┐
│                   거실                         │
│  ┌────┐                           ┌────────┐  │
│  │소파│      레고 놀이 영역        │  TV장  │  │
│  │    │     ◆◆ ◇◇ ◆◇ ◆          │        │  │
│  └────┘    ◇◆ ◆◆◇  ◇◆           └────────┘  │
│                                                │
│  ┌──────┐    🐕 강아지              ┌──────┐  │
│  │책장  │                           │식탁  │  │
│  │      │         👧 아이           │      │  │
│  └──────┘                           └──────┘  │
│                                                │
│  ───────── 복도 ──────────                     │
│  ┌─────────────┐  ┌──────────────┐             │
│  │  아이 방     │  │  주방        │             │
│  │  (레고 박스) │  │              │             │
│  └─────────────┘  └──────────────┘             │
└──────────────────────────────────────────────┘
```

### 2.2 오브젝트 목록

**레고 블록**
- 종류: 1x1, 1x2, 2x2, 2x4, 2x6, 2x8, 특수 블록(바퀴, 창문, 미니피규어)
- 색상: 빨강, 파랑, 노랑, 초록, 분홍, 갈색, 흰색, 검정
- 개수: 시나리오당 10~50개
- 상태: 바닥에 흩어짐, 일부 조립된 상태, 가구 아래/사이에 끼인 상태

**가구**
- 소파 (레고가 쿠션 사이에 끼임)
- 식탁 + 의자 (다리 사이에 레고)
- TV장 (아래 틈새에 레고)
- 책장 (선반 위에 올라간 레고)
- 러그/카펫 (레고가 파묻힘)
- 아이 방: 레고 정리 박스 (목적지)

**동적 장애물**
- 강아지: 돌아다님, 레고를 물고 감, 로봇 앞에 누움
- 아이: 놀고 있음, 레고를 만지고 있음, 뛰어다님
- 고양이: 선반 위에서 레고를 떨어뜨림 (옵션)

**로봇 (에이전트)**
- 가정용 로봇 팔 (6~7 DoF)
- 그리퍼: 레고 집기용
- 이동 베이스: 바퀴형 (가정 내 이동)
- 센서: RGB 카메라 (에고 시점), depth 센서

### 2.3 물리 환경 파라미터

```
방 크기:         5m × 7m (거실) + 3m × 4m (아이 방)
바닥 재질:       원목 마루 / 카펫 (마찰 계수 변동)
레고 개수:       Uniform(10, 50)
레고 위치:       바닥 랜덤 + 가구 근처 밀집
강아지 수:       0~2마리
아이 존재:       있음/없음 (시나리오별)
조명:            자연광 (시간대별) + 실내조명 (켜짐/꺼짐)
러그 위치/크기:  랜덤
가구 배치:       3~5가지 레이아웃 프리셋 + 미세 랜덤화
```

---

## 3. 전체 파이프라인

```
┌──────────────┐    ┌───────────────┐    ┌────────────────┐    ┌──────────────┐    ┌─────────────┐
│  환경 생성    │ →  │  에이전트 실행  │ →  │  데이터 기록    │ →  │  포맷 변환    │ →  │  모델 학습   │
│  (랜덤화)     │    │  (룰/RL)       │    │  (영상+액션)    │    │  (LeRobot)    │    │  (ACT/VLA)  │
└──────────────┘    └───────────────┘    └────────────────┘    └──────────────┘    └─────────────┘
       ↑                                                                                    │
       └────────────────────── 평가 & 실패 케이스 피드백 ──────────────────────────────────────┘
```

---

## 4. 일반 케이스 vs 롱테일 케이스

### 4.1 일반 케이스 (Normal Distribution — ~60%)

**바닥 줍기 (기본)**
- 마루 바닥에 레고 10~20개 흩어져 있음
- 장애물 없음, 직선 경로로 접근 가능
- 집어서 레고 박스에 넣기

**순차 수거**
- 여러 레고를 효율적 경로로 순서대로 수거
- 가까운 것부터, 또는 색상별 그룹핑

**단순 이동 후 수거**
- 방 한쪽에서 반대쪽으로 이동 후 수거
- 가구 사이 열린 통로로 이동

### 4.2 롱테일 케이스 — 강아지편 (~10%)

**강아지가 경로를 막음**
- 강아지가 로봇 이동 경로 한가운데 누워있음
- 우회 경로를 찾거나 대기해야 함
- 난이도: ★★☆

**강아지가 레고를 물고 감**
- 로봇이 접근하려는 레고를 강아지가 먼저 물어감
- 타겟이 이동하는 상황에서 재계획 필요
- 난이도: ★★★

**강아지가 로봇을 따라다님**
- 로봇이 이동할 때마다 강아지가 따라와서 발밑에 위치
- 충돌 회피하며 작업 지속
- 난이도: ★★☆

**강아지가 레고 박스를 뒤엎음**
- 정리한 레고가 다시 쏟아짐
- 재수거 판단 필요
- 난이도: ★★★

**강아지 장난감과 레고 구분**
- 바닥에 강아지 장난감(뼈다귀, 공)과 레고가 섞여 있음
- 레고만 선별해서 수거해야 함
- 난이도: ★★☆

### 4.3 롱테일 케이스 — 아이편 (~10%)

**아이가 레고로 놀고 있는 중**
- 아이가 일부 레고로 무언가를 만들고 있음
- 아이가 사용 중인 레고는 건드리면 안 됨
- 주변 흩어진 레고만 선별 수거
- 난이도: ★★★

**아이가 뛰어다님**
- 아이가 방 안을 뛰어다니며 예측 불가능한 이동
- 충돌 회피가 최우선 (안전)
- 난이도: ★★★

**아이가 정리한 레고를 다시 꺼냄**
- 레고 박스에 넣은 레고를 아이가 다시 꺼내서 뿌림
- 무한 루프 방지 판단 필요 (대기 또는 계속)
- 난이도: ★★☆

**아이가 레고를 건네줌**
- 아이가 로봇에게 레고를 직접 건네주는 상황
- 핸드오버(handover) 동작
- 난이도: ★★★

### 4.4 롱테일 케이스 — 가구/환경편 (~10%)

**소파 쿠션 사이 레고**
- 레고가 소파 쿠션 틈에 끼어있음
- 쿠션을 밀어내고 꺼내야 함 (2단계 동작)
- 난이도: ★★★

**가구 아래 레고**
- TV장, 소파, 책장 아래 좁은 틈에 레고가 들어감
- 팔을 낮게 뻗어서 꺼내야 함 (자세 제어)
- 난이도: ★★★

**카펫에 반쯤 묻힌 레고**
- 두꺼운 러그 위에 레고가 파묻혀 잘 안 보임
- 시각적으로 찾기 어려움
- 난이도: ★★☆

**레고가 다른 장난감과 섞임**
- 인형, 자동차, 퍼즐 조각 등과 레고가 섞여 있음
- 레고만 정확히 분류해서 수거
- 난이도: ★★☆

**의자 다리 사이 레고**
- 식탁 의자 다리 사이 좁은 공간에 레고
- 정밀한 그리퍼 제어 필요
- 난이도: ★★★

**선반 위 레고**
- 아이가 던져서 책장 선반 위에 올라간 레고
- 리프트 동작 필요 (높이 변화)
- 난이도: ★★☆

### 4.5 롱테일 케이스 — 로봇 자체 실패 (~5%)

**그리핑 실패 (미끄러짐)**
- 레고를 집었는데 이동 중 떨어뜨림
- 재시도 판단 필요
- 난이도: ★★☆

**센서 노이즈 / 부분 가림**
- 카메라에 빛 반사, 그림자로 레고가 잘 안 보임
- 시각 인식 견고성 테스트
- 난이도: ★★☆

**카펫-마루 경계 걸림**
- 바퀴형 베이스가 카펫 끝에 걸려서 이동 실패
- 경로 재계획 또는 다른 각도로 접근
- 난이도: ★★☆

### 4.6 롱테일 케이스 — 유사 물체 구분 (~5%)

**레고가 아닌 위험물**
- 작은 배터리, 동전, 단추 등이 레고와 섞여 있음
- 위험물은 별도 처리 (수거하되 다른 곳에 분류)
- 난이도: ★★★

**크기가 다른 물체들**
- 듀플로(큰 레고), 일반 레고, 테크닉 레고가 혼재
- 종류별 다른 그리핑 전략 필요
- 난이도: ★★☆

### 4.7 롱테일 케이스 — 복합 상황 (~10%)

**강아지 + 아이 동시 등장**
- 아이가 놀고 있고, 강아지가 돌아다니는 와중에 레고 정리
- 두 동적 장애물 동시 회피
- 난이도: ★★★

**조명 변화**
- 해질녘 자연광 감소, 그림자 변화
- 실내등 켜지면서 조명 조건 급변
- 시각 인식 견고성 테스트
- 난이도: ★★☆

**조립된 레고 구조물**
- 아이가 만들어둔 레고 작품이 바닥에 있음
- 분해하면 안 됨 (조립 상태 인식)
- 흩어진 낱개 레고만 수거
- 난이도: ★★★

**레고 밟기 직전 상황**
- 아이가 맨발로 레고를 향해 걸어오고 있음
- 우선순위를 바꿔서 아이 경로의 레고를 먼저 치워야 함
- 난이도: ★★★

**음식/음료 근처 레고**
- 식탁 위 쏟아진 음료 옆에 레고가 있음
- 젖은 영역 회피하면서 수거
- 난이도: ★★☆

**문이 닫힘**
- 레고 정리 박스가 있는 아이 방 문이 닫혀 있음
- 문 열기 → 이동 → 정리 (멀티스텝 계획)
- 난이도: ★★★

### 4.8 시나리오 분포 설계

```
전체 시나리오 = 1,000개

일반 케이스:              500개 (50%)
  - 바닥 줍기 기본:       250개 (레고 위치/개수/색상/조명 랜덤화)
  - 순차 수거:            150개
  - 단순 이동 후 수거:    100개

롱테일 — 강아지:          100개 (10%)
  - 경로 차단:             25개
  - 레고 물고 감:          25개
  - 따라다님:              20개
  - 박스 뒤엎음:           15개
  - 장난감 구분:           15개

롱테일 — 아이:            100개 (10%)
  - 놀고 있는 중:          30개
  - 뛰어다님:              30개
  - 다시 꺼냄:             20개
  - 건네줌:                20개

롱테일 — 가구/환경:       100개 (10%)
  - 소파 쿠션 사이:        20개
  - 가구 아래:             20개
  - 카펫 묻힘:             15개
  - 장난감 혼재:           15개
  - 의자 다리 사이:        15개
  - 선반 위:               15개

롱테일 — 로봇 실패:        50개 (5%)
  - 그리핑 미끄러짐:       20개
  - 센서 노이즈/가림:      15개
  - 카펫-마루 걸림:        15개

롱테일 — 유사 물체:        50개 (5%)
  - 위험물 구분:           25개
  - 크기별 레고 혼재:      25개

롱테일 — 복합:            100개 (10%)
  - 강아지+아이 동시:      15개
  - 조명 변화:             15개
  - 조립된 구조물:         15개
  - 밟기 직전:             15개
  - 음료 근처:             10개
  - 문 닫힘:               15개
  - 다중 방 이동:          15개
```

---

## 5. 에이전트 설계

### 5.1 룰 기반 Expert Agent (데이터 생성용)

```
상태 머신:

  SCANNING           →  레고 탐색 (시야 내 레고 감지)
    ↓
  APPROACHING        →  가장 가까운/우선순위 높은 레고로 이동
    ↓
  GRASPING           →  그리퍼로 레고 집기
    ↓
  TRANSPORTING       →  레고 박스로 이동
    ↓
  PLACING            →  박스에 넣기
    ↓
  SCANNING           →  다음 레고 탐색 (루프)

  예외 분기:
  AVOIDING_CHILD     →  아이 감지 시 정지 + 우회
  AVOIDING_DOG       →  강아지 감지 시 감속 + 우회
  WAITING            →  아이가 사용 중인 레고 근처에서 대기
  REACHING_UNDER     →  가구 아래 레고 꺼내기 (자세 변경)
  REACHING_HIGH      →  선반 위 레고 꺼내기 (리프트)
```

### 5.2 우선순위 로직

```
1. 안전: 아이/강아지와 충돌 절대 불가
2. 긴급: 아이 이동 경로의 레고 우선 수거 (밟기 방지)
3. 효율: 가까운 레고부터, 같은 영역 레고 묶어서 수거
4. 존중: 아이가 놀고 있는 레고는 건드리지 않음
5. 분류: 레고만 수거 (다른 장난감 제외)
```

### 5.3 로봇 액션 공간

```
이동 베이스 (3 DoF):
  - linear_velocity_x:   [-1.0, 1.0] m/s    전후진
  - linear_velocity_y:   [-1.0, 1.0] m/s    좌우 이동
  - angular_velocity:    [-1.0, 1.0] rad/s   회전

로봇 팔 (6 DoF):
  - joint_1 ~ joint_6:   각 관절 각도 변화율

그리퍼 (1 DoF):
  - gripper_open:        [0.0, 1.0]          0=닫힘, 1=열림

총: 10 DoF continuous action space
```

---

## 6. 다시점 동시 렌더링

**ego**: 로봇 머리 부착 1인칭 카메라 → 주요 학습 입력
**wrist**: 로봇 팔 손목 카메라 → 그리핑 정밀 제어
**overhead**: 방 천장 시점 → 전체 레이아웃 파악
**corner**: 방 모서리 고정 카메라 → 보안 카메라 시뮬레이션
**follow**: 3인칭 추적 카메라 → 데모 영상용

---

## 7. 데이터 기록 (매 프레임 동기화)

### 7.1 비디오

5시점 x MP4 (30fps), 해상도: 640x480 (학습용) / 1920x1080 (데모용)

### 7.2 액션 데이터 (`actions.jsonl`)

```json
{
  "frame": 150,
  "time": 5.0,
  "continuous_action": {
    "base_vx": 0.5,
    "base_vy": 0.0,
    "base_angular": 0.1,
    "joint_1": 0.02,
    "joint_2": -0.01,
    "joint_3": 0.03,
    "joint_4": 0.0,
    "joint_5": -0.02,
    "joint_6": 0.01,
    "gripper": 0.8
  },
  "state": "approaching",
  "robot_pose": {
    "base_x": 2.1, "base_y": 0.0, "base_z": -1.5,
    "base_heading": 1.2,
    "joint_angles": [0.1, -0.5, 0.3, 0.0, 0.2, -0.1],
    "gripper_state": 0.8,
    "holding_object": null
  },
  "targets": {
    "nearest_lego": {"dx": 0.3, "dz": -0.2, "color": "red", "size": "2x4"},
    "lego_box": {"dx": -3.0, "dz": 2.5}
  },
  "dynamic_obstacles": [
    {"type": "dog", "dx": 1.5, "dz": 0.8, "velocity": 0.3, "state": "walking"},
    {"type": "child", "dx": -1.0, "dz": -2.0, "velocity": 0.0, "state": "playing_lego"}
  ],
  "legos_remaining": 15,
  "legos_collected": 5
}
```

### 7.3 메타데이터 (`metadata.json`)

```json
{
  "scenario_id": "home_047",
  "category": "longtail_dog_blocks_path",
  "room_layout": "layout_A",
  "duration_seconds": 120,
  "duration_frames": 3600,
  "fps": 30,
  "resolution": {"width": 640, "height": 480},
  "camera_types": ["ego", "wrist", "overhead", "corner", "follow"],
  "environment": {
    "num_legos": 25,
    "lego_types": {"1x2": 5, "2x4": 10, "2x6": 5, "special": 5},
    "num_dogs": 1,
    "child_present": true,
    "child_state": "playing",
    "floor_type": "hardwood_with_rug",
    "lighting": "afternoon_natural",
    "furniture_layout": "preset_A_randomized"
  },
  "task_result": {
    "completed": true,
    "legos_collected": 20,
    "legos_skipped": 5,
    "skip_reason": "child_playing_with_them",
    "collisions": 0,
    "time_waiting": 15.2
  }
}
```

---

## 8. ACT/VLA 학습 파이프라인

### 8.1 데이터 포맷 변환

```
시뮬레이터 출력            →        학습 입력
─────────────                     ──────────
video_ego.mp4              →      observation.images.ego (numpy)
video_wrist.mp4            →      observation.images.wrist (numpy)
actions.jsonl              →      action (10 DoF continuous)
robot_pose in actions      →      observation.state (proprioception)
metadata.json              →      episode metadata

변환 포맷: LeRobot HDF5 (Hugging Face 표준)
```

### 8.2 ACT 학습 설정

```python
# LeRobot ACT config
policy: act
chunk_size: 20              # 20프레임(0.67초) 앞을 한번에 예측
batch_size: 64
lr: 1e-4
epochs: 500
backbone: ResNet18          # 이미지 인코더
transformer_layers: 6       # 액션 디코더
action_dim: 10              # base(3) + arm(6) + gripper(1)
camera_names: [ego, wrist]  # 2시점 입력
```

**입력**:
- `observation.images.ego`: 에고 카메라 프레임 (224x224)
- `observation.images.wrist`: 손목 카메라 프레임 (224x224)
- `observation.state`: robot_pose (heading, joint_angles, gripper)

**출력**:
- `action_chunk`: 향후 20스텝의 연속 액션
  - `[base_vx, base_vy, base_angular, j1~j6, gripper]` x 20 steps

### 8.3 VLA 확장 (OpenVLA)

ACT로 기본 동작 검증 후, 언어 조건 추가:
- "빨간 레고부터 치워줘"
- "소파 아래 레고를 꺼내줘"
- "아이가 놀고 있는 건 놔두고 나머지만 정리해"

---

## 9. GCP 학습 인프라

- **프로젝트**: ralphton (GCP)
- **VM**: ralphton-a100 (us-central1-a)
- **GPU**: NVIDIA A100-SXM4-40GB x1 (Spot)
- **비용**: ~$1.80/시간 (~2,500원/시간)
- **PyTorch**: 2.7, CUDA 12.8
- **예상 학습 시간**: ACT 1~3시간 / OpenVLA LoRA 10~15시간

---

## 10. 현재 진행 상황

### 완료

- GCP ralphton 프로젝트 생성 및 A100 VM 프로비저닝 (동작 확인됨)
- A100 40GB Spot VM: nvidia-smi 확인, PyTorch 2.7, CUDA 12.8
- 기존 레고 정리 게임 (Three.js, 브라우저 기반 클릭 게임)
- 레고 블록 3D 모델 (다양한 크기/색상)
- D2E 이론적 프레임워크 및 기술 스택 결정

### 미완료 (GAP)

**Phase 0 — Vertical Slice (최우선)**
- 최소 3D 환경 (빈 방 + 레고 5개)
- 로봇 에이전트 (이동 + 그리퍼) — cannon-es 물리 가능성 검증 필요
- Expert Agent (최소 상태 머신)
- 데이터 기록 (영상 + 액션 JSONL)
- LeRobot HDF5 변환
- ACT 학습 loss 확인

**Phase 1~2 — 환경 확장**
- 가정집 환경 3D 모델링 (거실, 아이 방, 가구)
- 연속 액션 데이터 기록 (10 DoF)
- 동적 장애물 (강아지, 아이) 행동 AI
- 다시점 렌더링 (5개 카메라)

**Phase 3~4 — 대량 생성 + 데모**
- 1,000개 시나리오 배치 생성 시스템
- ACT 학습 하이퍼파라미터 최적화
- 투자자/파트너 데모 영상

**Future**
- OpenVLA fine-tuning (언어 지시)
- D2E VAPT 파이프라인 적용 (Sim-to-Real 전이)
- 실제 가정용 로봇 팔 연동

---

## 11. 로드맵 (Vertical Slice 우선)

핵심 원칙: **전체 파이프라인을 최소 단위로 먼저 관통한 뒤, 점진적으로 복잡도를 높인다.**

### Phase 0: Vertical Slice — 전체 관통 검증 (1주)

> 가장 중요한 단계. 여기서 막히면 전체 방향 재검토.

1. **최소 환경 구성**: 빈 방 하나 + 레고 5개 + 레고 박스 1개 (Three.js)
2. **최소 로봇**: 이동 베이스 + 단순 그리퍼 (물리 정확도보다 동작 가능성 우선)
3. **최소 Expert Agent**: 가장 가까운 레고로 이동 → 집기 → 박스에 넣기
4. **데이터 기록**: 1개 에피소드의 영상(ego 시점) + 연속 액션 JSONL 저장
5. **LeRobot 변환**: JSONL+MP4 → HDF5 변환 확인
6. **ACT 학습 테스트**: 50~100 에피소드로 loss가 떨어지는지 확인 (A100 VM)

**Phase 0 성공 기준**: 시뮬레이터 → 데이터 → 학습 → loss 감소가 확인되면 통과

### Phase 1: 기본 환경 확장 (1~2주)

7. 가정집 거실 3D 환경 모델링 (가구 배치 포함)
8. 로봇 팔 DoF 확장 (6DoF + 그리퍼)
9. 레고 종류/색상 다양화 (10~30개)
10. 다시점 렌더링 추가 (ego + wrist + overhead)
11. 200개 에피소드 생성 → ACT 재학습 → 성능 확인

### Phase 2: 동적 환경 + 롱테일 (2~3주)

12. 강아지 행동 AI (간단한 상태 머신: 걷기, 눕기, 경로 차단)
13. 아이 행동 AI (놀기, 뛰기, 레고 만지기)
14. 가구 인터랙션 (소파 아래, 의자 사이 레고)
15. 롱테일 시나리오 생성기 구현
16. 500개 에피소드 (일반 + 롱테일) → ACT 재학습

### Phase 3: 대량 생성 + 학습 최적화 (1~2주)

17. 1,000개 시나리오 배치 생성 (일반 500 + 롱테일 500)
18. 데이터 품질 검수 및 통계 분석
19. ACT 학습 하이퍼파라미터 튜닝
20. 시뮬레이션 내 평가 (새 시나리오에서 학습된 policy 실행)
21. 실패 분석 → 추가 데이터 → 재학습 (피드백 루프)

### Phase 4: 데모 제작 (1주)

22. 투자자/파트너 시연용 5개 데모 시나리오 정제
23. 고해상도(1920x1080) 데모 영상 렌더링
24. 성과 지표 정리 (성공률, 수거 효율, 안전 회피 등)

### Phase 5: 확장 (장기)

25. OpenVLA fine-tuning (언어 지시 추가)
26. 더 복잡한 가정집 시나리오 (다층 구조, 여러 방)
27. Sim-to-Real 테스트 (D2E VAPT 파이프라인 적용)

### 리스크 체크포인트

- **Phase 0 실패 시**: cannon-es로 로봇 팔 집기가 불가능하면 → 물리를 단순화하거나 프록시 방식(proximity 기반 "집힌 것으로 간주") 도입
- **Phase 1 실패 시**: headless 렌더링이 안 되면 → Puppeteer headless Chrome 또는 Xvfb 대안 검토
- **Phase 2 실패 시**: 동적 장애물 AI가 너무 복잡하면 → 미리 녹화된 경로 재생 방식으로 단순화

---

## 12. 데모 시나리오 (투자자/파트너 시연용)

### 데모 1: 기본 능력 — "깔끔한 정리"
- 거실 바닥에 레고 20개 흩어져 있음
- 로봇이 효율적 경로로 하나씩 집어서 박스에 정리
- 포인트: 기본 pick-and-place 능력

### 데모 2: 안전 의식 — "아이와 함께"
- 아이가 레고로 놀고 있는 옆에서 정리
- 아이가 사용 중인 레고는 건드리지 않음
- 아이가 뛰어올 때 정지
- 포인트: 안전 인식, 상황 판단

### 데모 3: 적응력 — "강아지가 방해해도"
- 강아지가 돌아다니면서 경로를 막음
- 강아지가 레고를 물고 감
- 로봇이 우회하고 재계획
- 포인트: 동적 환경 대응

### 데모 4: 문제 해결 — "숨은 레고 찾기"
- 소파 아래, 카펫에 묻힌, 의자 사이 레고
- 다양한 자세와 전략으로 수거
- 포인트: 복잡한 환경에서의 조작 능력

### 데모 5 (OpenVLA): 언어 지시 — "말로 시키기"
- "빨간 레고만 치워줘" → 빨간색만 수거
- "소파 아래 먼저 해줘" → 우선순위 변경
- 포인트: VLA의 언어 이해 능력

---

## 13. 기술 스택

**이론적 기반**: D2E (Desktop to Embodied AI) — WoRV AI
**3D 렌더링**: Three.js + headless-gl (서버사이드) — AI 코드 생성에 최적화
**물리 엔진**: cannon-es — 게임 수준 물리, D2E 패러다임에 충분
**비디오 인코딩**: FFmpeg (H.264, 30fps)
**에이전트**: 룰 기반 Expert Agent (데이터 생성용)
**환경 생성**: Claude/LLM이 Three.js 코드 자동 생성
**3D 모델**: GLTF/GLB
**학습 프레임워크**: LeRobot (HuggingFace) — ACT 내장
**VLA 모델**: OpenVLA / OpenVLA-OFT
**학습 인프라**: GCP A100 VM (ralphton)
**데이터 포맷**: LeRobot HDF5 (학습) / JSONL+MP4 (원본)

---

## 14. Sim-to-Real 전이 전략 (투자자 대응용)

"시뮬레이션 결과가 실제 로봇에서도 동작하나?"에 대한 답변 근거:

**D2E 검증 결과**
- LIBERO 조작 작업: 96.6% 성공률 (게임 데이터 → 로봇 전이)
- 실제 SO101 로봇: 70% → 80% 향상
- YouTube 게임플레이 1,000시간+ 자동 라벨링으로 스케일 가능

**전이 전략 (단계적)**
- **단기**: 시뮬레이션 내 데모로 모델 능력 증명
- **중기**: D2E VAPT(시각-행동 사전학습) 적용하여 실제 로봇 fine-tuning
- **장기**: 소량의 실제 로봇 데이터(50~100 에피소드)로 sim-to-real gap 해소

**타겟 하드웨어 (후보)**
- SO101 (오픈소스, 저비용, D2E 논문에서 검증됨)
- WidowX (연구용 표준, OpenVLA 논문에서 검증됨)
- 가정용 로봇 팔 (추후 결정)

---

## 15. 참고 논문

- **D2E** (WoRV AI, 2025) — Desktop to Embodied AI, 게임 데이터 → 로봇 전이 (https://worv-ai.github.io/d2e/)
- **ACT** (Zhao et al., 2023) — Action Chunking + Transformer 로봇 조작
- **OpenVLA** (Kim et al., 2024) — Vision-Language-Action 7B 모델
- **OpenVLA-OFT** (2025) — 추론 25~50배 속도 향상
- **RT-2** (Brohan et al., 2023) — Vision-Language-Action 통합 모델
- **Domain Randomization** (Tobin et al., 2017) — 시뮬레이션 랜덤화 Sim-to-Real
- **LeRobot** (HuggingFace, 2024) — 오픈소스 로봇 학습 프레임워크

---

*문서 작성: 2026-02-28 (v2 — D2E 기반 + Vertical Slice 로드맵 반영)*
*프로젝트: ralphton*
*GCP 프로젝트: ralphton (seo@repact.ai.kr)*
*VM: ralphton-a100 (us-central1-a, A100 40GB Spot)*
